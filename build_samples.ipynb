{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function running with parameter regen: False\n",
      "Loading saved file from data/processed/sample_sequence_dict.pkl\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/w3/mpbw0w2d5239lm7g55vdjzp80000gn/T/ipykernel_45766/1050603982.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[0mbuilder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBuildSamples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjaad_object\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mJAAD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/jaad\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_sequence_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/w3/mpbw0w2d5239lm7g55vdjzp80000gn/T/ipykernel_45766/1050603982.py\u001b[0m in \u001b[0;36mgenerate_sequence_samples\u001b[0;34m(self, image_set, window, visualize_inner_func, save, regen, **opts)\u001b[0m\n\u001b[1;32m    414\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_sequence_dict_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m                     \u001b[0mparsed_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m                 \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparsed_sequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sequences'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/w3/mpbw0w2d5239lm7g55vdjzp80000gn/T/ipykernel_45766/1050603982.py\u001b[0m in \u001b[0;36m_make_features\u001b[0;34m(self, sequence_dict)\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0mbody\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"numpy_checkpoint_{str(min(range_list))}-{str(max(range_list))}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'data/processed/numpy_feature_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m         \u001b[0mprocess_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlists_to_work_on\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/pedestrians/lib/python3.9/site-packages/tqdm/contrib/concurrent.py\u001b[0m in \u001b[0;36mprocess_map\u001b[0;34m(fn, *iterables, **tqdm_kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mtqdm_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mtqdm_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lock_name\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"mp_lock\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_executor_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mProcessPoolExecutor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0miterables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtqdm_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniforge3/envs/pedestrians/lib/python3.9/site-packages/tqdm/contrib/concurrent.py\u001b[0m in \u001b[0;36m_executor_map\u001b[0;34m(PoolExecutor, fn, *iterables, **tqdm_kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mmap_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpool_kwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0miterables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmap_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/pedestrians/lib/python3.9/concurrent/futures/process.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, fn, timeout, chunksize, *iterables)\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"chunksize must be >= 1.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m         results = super().map(partial(_process_chunk, fn),\n\u001b[0m\u001b[1;32m    727\u001b[0m                               \u001b[0m_get_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0miterables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m                               timeout=timeout)\n",
      "\u001b[0;32m~/miniforge3/envs/pedestrians/lib/python3.9/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, fn, timeout, chunksize, *iterables)\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m         \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0miterables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;31m# Yield must be hidden in closure so that the futures are submitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/pedestrians/lib/python3.9/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m         \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0miterables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;31m# Yield must be hidden in closure so that the futures are submitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/pedestrians/lib/python3.9/concurrent/futures/process.py\u001b[0m in \u001b[0;36m_get_chunks\u001b[0;34m(chunksize, *iterables)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_get_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0miterables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;34m\"\"\" Iterates over zip()ed iterables in chunks. \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0miterables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import typing\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import yaml\n",
    "from data.jaad.jaad_data import JAAD\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "from scipy import signal\n",
    "from tqdm import tqdm\n",
    "from multiprocessing.pool import ThreadPool as Pool\n",
    "from tqdm.contrib.concurrent import process_map \n",
    "\n",
    "from src.data_utils import JaadDatabase\n",
    "\n",
    "\n",
    "class BuildSamples(JaadDatabase):\n",
    "    def __init__(\n",
    "        self,\n",
    "        jaad_object: object,\n",
    "        filename: typing.Union[str, bytes, os.PathLike] = \"jaad_database.pkl\",\n",
    "        regen=False,\n",
    "        processed_dirpath=\"data/processed\",\n",
    "    ) -> None:\n",
    "        super().__init__(jaad_object, filename=filename, regen=regen, processed_dirpath=processed_dirpath)\n",
    "\n",
    "        self.db = self.read_from_pickle(\"data/processed/jaad_database.pkl\")\n",
    "        self.sequence_dict = None\n",
    "\n",
    "    def _generate_raw_sequence(self, image_set=\"all\", **opts):\n",
    "        \"\"\"\n",
    "        Generates pedestrian tracks\n",
    "        :param image_set: the split set to produce for. Options are train, test, val.\n",
    "        :param opts:\n",
    "                'fstride': Frequency of sampling from the data.\n",
    "                'sample_type': Whether to use 'all' pedestrian annotations or the ones\n",
    "                                    with 'beh'avior only.\n",
    "                'subset': The subset of data annotations to use. Options are: 'default': Includes high resolution and\n",
    "                                                                                        high visibility videos\n",
    "                                                                        'high_visibility': Only videos with high\n",
    "                                                                                            visibility (include low\n",
    "                                                                                            resolution videos)\n",
    "                                                                        'all': Uses all videos\n",
    "                'height_rng': The height range of pedestrians to use.\n",
    "                'squarify_ratio': The width/height ratio of bounding boxes. A value between (0,1]. 0 the original\n",
    "                                        ratio is used.\n",
    "                'data_split_type': How to split the data. Options: 'default', predefined sets, 'random', randomly split the data,\n",
    "                                        and 'kfold', k-fold data split (NOTE: only train/test splits).\n",
    "                'seq_type': Sequence type to generate. Options: 'trajectory', generates tracks, 'crossing', generates\n",
    "                                tracks up to 'crossing_point', 'intention' generates tracks similar to human experiments\n",
    "                'min_track_size': Min track length allowable.\n",
    "                'random_params: Parameters for random data split generation. (see _get_random_pedestrian_ids)\n",
    "                'kfold_params: Parameters for kfold split generation. (see _get_kfold_pedestrian_ids)\n",
    "        :return: Sequence data in the form\n",
    "        ---------------------------\n",
    "        keys = dict_keys(['image', 'pid', 'bbox', 'center', 'occlusion', 'intent'])\n",
    "\n",
    "        {'image': [['../data/jaad/images/video_0001/00000.png',\n",
    "                    '../data/jaad/images/video_0001/00001.png',\n",
    "                    '../data/jaad/images/video_0001/00002.png',\n",
    "                    '../data/jaad/images/video_0001/00003.png',\n",
    "                    '../data/jaad/images/video_0001/00004.png',...\n",
    "        'pid': [[['0_1_2b'],\n",
    "                ['0_1_2b'],\n",
    "                ['0_1_2b'],\n",
    "                ['0_1_2b'],\n",
    "                ['0_1_2b'],\n",
    "                ['0_1_2b'],\n",
    "                ['0_1_2b'],\n",
    "                ['0_1_2b'],\n",
    "                ['0_1_2b'],\n",
    "                ['0_1_2b'],\n",
    "                ['0_1_2b'],\n",
    "                ['0_1_2b'],\n",
    "                ['0_1_2b'],...\n",
    "        ---------------------------\n",
    "        \"\"\"\n",
    "        params = {\n",
    "            \"fstride\": 1,\n",
    "            \"sample_type\": \"all\",  # 'beh'\n",
    "            \"subset\": \"default\",\n",
    "            \"height_rng\": [0, float(\"inf\")],\n",
    "            \"squarify_ratio\": 0,\n",
    "            \"data_split_type\": \"default\",  # kfold, random, default\n",
    "            \"seq_type\": \"intention\",\n",
    "            \"min_track_size\": 15,\n",
    "            \"random_params\": {\"ratios\": None, \"val_data\": True, \"regen_data\": False},\n",
    "            \"kfold_params\": {\"num_folds\": 5, \"fold\": 1},\n",
    "        }\n",
    "        assert all(k in params for k in opts.keys()), \"Wrong option(s).\" \"Choose one of the following: {}\".format(\n",
    "            list(params.keys())\n",
    "        )\n",
    "        params.update(opts)\n",
    "        return self.jaad_object.generate_data_trajectory_sequence(image_set, **params)\n",
    "\n",
    "    def _raw_sequence_transformer(self, sequence_data: dict, window, **opts):\n",
    "        \"\"\"\n",
    "        Parses a single sequence from _generate_raw_sequence (by calling generate_sequence_samples)\n",
    "        :param sequence_data: The sequence data to parse. Comes in the form dict(image, pid, intent)\n",
    "        :param window: The window size to use\n",
    "        \"\"\"\n",
    "\n",
    "        params = {\n",
    "            \"savgol_window_size\": 11,\n",
    "            \"savgol_polyorder\": 3,\n",
    "        }\n",
    "        params.update(opts)\n",
    "\n",
    "        # unpack dict(image, pid)\n",
    "        image_list, pid_list, intent = sequence_data[\"image\"], sequence_data[\"pid\"], sequence_data[\"intent\"]\n",
    "\n",
    "        # PID is the same along the array, then we just need one\n",
    "        pid = pid_list[0][0]\n",
    "\n",
    "        # same for video_name\n",
    "        video_name = image_list[0].split(\"/\")[-2]\n",
    "\n",
    "        # get rolling windows\n",
    "        sliding_windows_image_array = sliding_window_view(image_list, window)\n",
    "        sliding_windows_intent_array = sliding_window_view(intent, window, 0)\n",
    "\n",
    "        sliding_window_frame_id = [\n",
    "            pd.Series(arr).str.extract(r\"(\\d{5})\").astype(int).to_numpy().flatten()\n",
    "            for arr in sliding_windows_image_array\n",
    "        ]  # array([['../data/jaad/images/video_0001/00000.png', '../data/jaad/images/video_0001/00001.png' => [array([0, 1, 2, 3, 4...]), array([1, 2, 3, 4, 5...])....\n",
    "\n",
    "        # frames of the pid\n",
    "        pid_frames = self.db[video_name][\"ped_annotations\"][pid][\"frames\"]\n",
    "        skeletons = self.db[video_name][\"ped_annotations\"][pid][\"skeleton_keypoints\"]\n",
    "\n",
    "        # apply savgol filter\n",
    "        skeletons = self._apply_savgol_filter(\n",
    "            skeletons, window_size=params[\"savgol_window_size\"], polyorder=params[\"savgol_polyorder\"]\n",
    "        )\n",
    "\n",
    "        array_with_sequences = []\n",
    "        for array_ix, array in enumerate(\n",
    "            sliding_window_frame_id\n",
    "        ):  # an array contain the ids of the frames for that sliding window\n",
    "            skeletons_single_sequence = []\n",
    "            confidence_single_sequence = []\n",
    "            for ix in array:\n",
    "                try:\n",
    "                    frame_skeleton = skeletons[pid_frames.index(ix), :, :]\n",
    "                except IndexError:\n",
    "                    frame_skeleton = None\n",
    "                if (frame_skeleton is None) or (np.isnan(frame_skeleton).all()):\n",
    "                    break\n",
    "                skeletons_single_sequence.append(frame_skeleton[:, :2])\n",
    "                confidence_single_sequence.append(frame_skeleton[:, 2])\n",
    "            assert all(\n",
    "                len(lst[\"frame_sequence\"]) == len(array) for lst in array_with_sequences\n",
    "            ), \"Skeleton length does not match array length\"\n",
    "            if frame_skeleton is not None:\n",
    "                data = {\n",
    "                    \"video_name\": video_name,\n",
    "                    \"pid\": pid,\n",
    "                    \"frame_sequence\": array,\n",
    "                    \"skeleton_sequence\": skeletons_single_sequence,\n",
    "                    \"confidence\": confidence_single_sequence,\n",
    "                    \"intent\": sliding_windows_intent_array[array_ix].flatten(),\n",
    "                }\n",
    "                array_with_sequences.append(data)\n",
    "\n",
    "        return array_with_sequences\n",
    "\n",
    "    def _interpolate_array_column(self, arr):\n",
    "        \"\"\"\n",
    "        Interpolates a numpy array column\n",
    "        :param arr: Numpy array\n",
    "        :return: Numpy array with interpolated columns\n",
    "        \"\"\"\n",
    "        args_where_zero = np.where(arr == 0)\n",
    "        arr_copy = arr.copy()\n",
    "\n",
    "        series = pd.Series(arr_copy)\n",
    "        series[series == 0] = np.NaN\n",
    "\n",
    "        series = series.interpolate().interpolate(method=\"bfill\")\n",
    "        arr_copy[args_where_zero] = series.iloc[args_where_zero].values\n",
    "\n",
    "        return arr_copy\n",
    "\n",
    "    def _apply_savgol_filter(self, skeletons, window_size=11, polyorder=3, interpolate=True):\n",
    "        \"\"\"\n",
    "        Applies Savgol filter to the array\n",
    "        :param skeletons: The array of frames containing skeletons\n",
    "        :param window_size: The window size\n",
    "        :param polyorder: The polynomial order\n",
    "        :return: The filtered array\n",
    "        \"\"\"\n",
    "        for i, _arr in enumerate(skeletons):\n",
    "            if _arr is None:\n",
    "                _arr = np.empty((25, 3))\n",
    "                _arr[:] = np.NaN\n",
    "            skeletons[i] = _arr\n",
    "\n",
    "        stack = np.vstack(skeletons).reshape(-1, 75)\n",
    "        for i in range(stack.shape[1]):\n",
    "\n",
    "            # interpolate where the values are 0\n",
    "            if interpolate and np.any(stack[:, i] == 0):\n",
    "                stack[:, i] = self._interpolate_array_column(stack[:, i])\n",
    "\n",
    "            stack[:, i] = signal.savgol_filter(stack[:, i], window_size, polyorder, mode=\"nearest\")  # run the filter\n",
    "\n",
    "            # fix getting negative values\n",
    "            zero_negative_value_indices = (np.where(stack[:, i] <= 0))[0]\n",
    "            # if there are negative value, check previous value, if its positive use it, if its negative, set ot 0.\n",
    "            # If there is no previous value, use next value with same procedure.\n",
    "            for arg in zero_negative_value_indices:\n",
    "                try:\n",
    "                    if stack[arg - 1, i] > 0:\n",
    "                        stack[arg, i] = stack[arg - 1, i]\n",
    "                    else:\n",
    "                        stack[arg, i] = 0\n",
    "                except IndexError:\n",
    "                    if stack[arg + 1, i] > 0:\n",
    "                        stack[arg, i] = stack[arg + 1, i]\n",
    "                    else:\n",
    "                        stack[arg, i] = 0\n",
    "\n",
    "            non_nan_array = stack[:, i][~np.isnan(stack[:, i])]\n",
    "            if not np.isnan(non_nan_array).all():\n",
    "                assert non_nan_array.min() >= 0, \"Negative values in the array\"\n",
    "\n",
    "        stack = stack.reshape(-1, 25, 3)\n",
    "        return stack\n",
    "\n",
    "    def _parse_raw_sequence(self, sequence_data, **opts):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            sequence_data (dict): sequence_data from _generate_raw_sequence\n",
    "\n",
    "        Returns:\n",
    "            dict: In the shape of:\n",
    "            [\n",
    "               [\n",
    "                   {\n",
    "                       'video_name': 'video_0001',\n",
    "                        'pid': '0_1_2b',\n",
    "                        'frame_sequence': array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14]), -> single array\n",
    "                        'skeleton_sequence': [array([[1447.6762,  675.5998],...array([[...]])] -> multiple arrays, one per frame of frame_sequence.\n",
    "                        'confidence': [array([0.796439, 0.76417...],...array([[0.....]])], -> multiple arrays, one per frame of frame_sequence,\n",
    "                        'intent': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) -> single array, one element per frame\n",
    "                    },\n",
    "\n",
    "                    {\n",
    "                        'video_name': 'video_0001',\n",
    "                        'pid': '0_1_2b',\n",
    "                        'frame_sequence': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15]), -> NOTE this is what change between elements of the list !NOTE\n",
    "                        'skeleton_sequence': [array([[1450.5655,  675.1043],...\n",
    "                    },\n",
    "                    ...\n",
    "                ] NOTE this whole list was just for video_0001 and pedestrian_id 0_1_2b...,\n",
    "                [\n",
    "                    NEW LIST FOR video_0001 and pedestrian_id 0_1_3b\n",
    "                ],\n",
    "                [\n",
    "                    NEW LIST FOR video_0002 and pedestrian_id ...\n",
    "                ],\n",
    "        \"\"\"\n",
    "\n",
    "        params = {}\n",
    "        params.update(opts)\n",
    "\n",
    "        # if visualize_inner_func is True, we want to print the sequence data to look at it\n",
    "        print(\"+\" * 30, \"Print raw sequence data\", sequence_data, \"+\" * 30, sep=\"\\n\") if params[\n",
    "            \"visualize_inner_func\"\n",
    "        ] else None\n",
    "\n",
    "        sequence_dict = {\n",
    "            \"sequences_count\": 0,\n",
    "            \"pos_sequences\": 0,\n",
    "            \"sequences\": [],\n",
    "        }\n",
    "\n",
    "        print(\"-\" * 70, \"Parsing sequence data\", sep=\"\\n\")\n",
    "        for ith_sample in tqdm(range(len(sequence_data[\"image\"]))):\n",
    "            sample_data = {\n",
    "                \"image\": sequence_data[\"image\"][ith_sample],\n",
    "                \"pid\": sequence_data[\"pid\"][ith_sample],\n",
    "                \"intent\": sequence_data[\"intent\"][ith_sample],\n",
    "            }\n",
    "            parsed_sample = self._raw_sequence_transformer(\n",
    "                sequence_data=sample_data,\n",
    "                window=params[\"min_track_size\"],\n",
    "                savgol_window_size=params[\"savgol_window_size\"],\n",
    "                savgol_polyorder=params[\"savgol_polyorder\"],\n",
    "            )\n",
    "            sequence_dict[\"sequences\"].append(parsed_sample)\n",
    "            sequence_dict[\"sequences_count\"] += len(parsed_sample)\n",
    "            for i in parsed_sample:\n",
    "                if i[\"intent\"][0] == 1:\n",
    "                    sequence_dict[\"pos_sequences\"] += 1\n",
    "\n",
    "        return sequence_dict\n",
    "    \n",
    "    def _split_list(self, a, n):\n",
    "        k, m = divmod(len(a), n)\n",
    "        return (a[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n))\n",
    "\n",
    "    def _make_features(self, sequence_dict):\n",
    "        \"\"\"\n",
    "        Each sequence is a list of dictionaries, each dictionary is a frame.\n",
    "        Each sequence represents a pedestrian id and its video. E.g. sequence_dict[0] refers to pedestrian id 0_1_2b and video_0001. sequence_dict[1] refers to pedestrian id 0_1_3b and video_0001. Each dictionary on a sequence contains the name of the video, pid, a frame sequence (list of frames), a skeleton sequence (list of skeletons), the intent (array) and the confidence(array). Each skeleton is a list of 25 2D points, each point is a list of x, y coordinates.\n",
    "\n",
    "        The next dictionary on the sequence will be the same but for the next frame sequence. For example dictionary[0] will have the frame sequence [0, 1....,14], dictionary[1] will have the frame sequence [1, 2....,15], dictionary[2] will have the frame sequence [2, 3....,16], and so on.\n",
    "        \"\"\"\n",
    "                \n",
    "        \n",
    "        num_workers = 8\n",
    "        lists_to_work_on = list(self._split_list(range(len(sequence_dict)), num_workers))\n",
    "        \n",
    "    \n",
    "        def run(range_list):\n",
    "            general_counter = 0\n",
    "            feature_array = np.empty((0)) \n",
    "            for n in range_list:\n",
    "                for j in range(len((sequence_dict[n]))):\n",
    "                    n_pid_seq = sequence_dict[n][j]\n",
    "\n",
    "                    feature_dict = {}\n",
    "                    feature_counter = 0\n",
    "\n",
    "                    for i, skeleton in enumerate(n_pid_seq['skeleton_sequence']):\n",
    "                        body = BodyBuilder(skeleton)\n",
    "                        frame = n_pid_seq['frame_sequence'][i]\n",
    "                        \n",
    "                        # angle combinations\n",
    "                        comb_names = body.get_angle_combinations()[2]\n",
    "                        comb_values = body.get_angle_combinations()[0]\n",
    "                        angles_dict = dict(zip(comb_names, comb_values))\n",
    "                        for k, v in angles_dict.items():\n",
    "                            # [feature_0]_angle_LShoulder_RShoulder_LWrist_frame_0\n",
    "                            feature_dict[f\"[feature-{feature_counter}]_ANGLE_[{k[0]}-{k[1]}{k[2]}]_[frame-{frame}]\"] = v \n",
    "                            feature_counter += 1\n",
    "\n",
    "                        ## cosine features\n",
    "                        cosine_dict = body.get_cosine_features()\n",
    "                        for k, v in cosine_dict.items():\n",
    "                            feature_dict[f\"[feature-{feature_counter}]_COSINE_[{k}]_[frame-{frame}]\"] = v \n",
    "                            feature_counter += 1\n",
    "\n",
    "                        ## position relative to frame features\n",
    "                        position_dict = dict(zip(body.get_position_features().index, body.get_position_features().values))\n",
    "                        for k, v in position_dict.items():\n",
    "                            feature_dict[f\"[feature-{feature_counter}]_POSITION-ON-FRAME-X_[{k}]_[frame-{frame}]\"] = v[0]\n",
    "                            feature_dict[f\"[feature-{feature_counter}]_POSITION_ON_FRAME-Y_[{k}]_[frame-{frame}]\"] = v[1]\n",
    "                            feature_counter += 2\n",
    "                    \n",
    "                        ## normalized_keypoints\n",
    "                        normalized_keypoint_dict = dict(zip(body.get_normalized_body_parts_df().index, body.get_normalized_body_parts_df().values))\n",
    "                        \n",
    "                        for k, v in normalized_keypoint_dict.items():\n",
    "                            feature_dict[f\"[feature-{feature_counter}]_NORMALIZED-KEYPOINT-X_[{k}]_[frame-{frame}]\"] = v[0]\n",
    "                            feature_dict[f\"[feature-{feature_counter}]_NORMALIZED-KEYPOINT-Y_[{k}]_[frame-{frame}]\"] = v[1]\n",
    "                            feature_counter += 2\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    vals = np.fromiter(feature_dict.values(), dtype=float)\n",
    "                    if general_counter == 0:\n",
    "                        feature_array = np.append(feature_array, vals)\n",
    "                    else:\n",
    "                        feature_array = np.vstack((feature_array, vals))\n",
    "                    general_counter += 1\n",
    "                    \n",
    "            body.save_numpy(feature_array, f\"numpy_checkpoint_{str(min(range_list))}-{str(max(range_list))}\", dir_path='data/processed/numpy_feature_array')\n",
    "        \n",
    "        process_map(run, lists_to_work_on, num_workers)            \n",
    "\n",
    "\n",
    "    def generate_sequence_samples(\n",
    "        self, image_set=\"all\", window=15, visualize_inner_func=False, save=True, regen=False, **opts\n",
    "    ):\n",
    "\n",
    "        \"\"\"\n",
    "        Calls _generate_raw_sequence to generate sequence data.\n",
    "        :param image_set: the split set to produce for. Options are train, test, val.\n",
    "\n",
    "        :return: Sequence data\n",
    "        \"\"\"\n",
    "        params = {\n",
    "            \"sample_type\": \"beh\",\n",
    "            \"height_rng\": [60, float(\"inf\")],\n",
    "            \"min_track_size\": window,\n",
    "        }\n",
    "        assert all(k in params for k in opts.keys()), \"Wrong option(s).\" \"Choose one of the following: {}\".format(\n",
    "            list(params.keys())\n",
    "        )\n",
    "\n",
    "        # SECTION: LOAD FILE OR RUN FUNCTIONS FOR PARSED SEQUENCES DICT\n",
    "        # load sample_sequence_dict if exist and regen is false\n",
    "        sample_sequence_dict_path = Path(self.processed_dirpath) / \"sample_sequence_dict.pkl\"\n",
    "        print(f\"Function running with parameter regen: {regen}\")\n",
    "        if not regen:\n",
    "            if sample_sequence_dict_path.exists():\n",
    "                print(f\"Loading saved file from {str(sample_sequence_dict_path)}\")\n",
    "                with open(str(sample_sequence_dict_path), \"rb\") as file:\n",
    "                    parsed_sequences = pickle.load(file)\n",
    "                features = self._make_features(sequence_dict=parsed_sequences['sequences'])\n",
    "                return features\n",
    "            else:\n",
    "                print(f\"No previous pickle file found on {str(sample_sequence_dict_path)}\", \"Generating...\", sep=\"\\n\")\n",
    "\n",
    "        if regen:  # Print notice nto user\n",
    "            print(f\"Forcing regeneration of {sample_sequence_dict_path}\")\n",
    "\n",
    "        # run functions\n",
    "        sequence_data = self._generate_raw_sequence(image_set, **params)\n",
    "        parsed_sequences = self._parse_raw_sequence(\n",
    "            sequence_data=sequence_data,\n",
    "            **params,\n",
    "            visualize_inner_func=visualize_inner_func,\n",
    "            savgol_window_size=7,\n",
    "            savgol_polyorder=3,\n",
    "        )\n",
    "\n",
    "        if save:\n",
    "            with open(\"data/processed/sample_sequence_dict.pkl\", \"wb\") as file:\n",
    "                pickle.dump(parsed_sequences, file)\n",
    "        # !SECTION\n",
    "        features = self._make_features(sequence_dict=parsed_sequences['sequences'])\n",
    "\n",
    "\n",
    "        return features  # return attribute\n",
    "\n",
    "\n",
    "class BodyBuilder:\n",
    "    def __init__(self, arr: np.ndarray) -> None:\n",
    "        self.arr = arr\n",
    "\n",
    "        self.body_yaml_path = \"data/helper_data/body_25b_parts.yaml\"\n",
    "        self.body_yaml_dict = self._load_body_yaml_dict()\n",
    "\n",
    "        self.body_parts_coordinates = self._map_to_dict(self.arr, self.body_yaml_dict)\n",
    "\n",
    "        self.length_body = None\n",
    "\n",
    "        self._compute_length_body()\n",
    "\n",
    "        # dataframe option\n",
    "        self.body_parts_df = pd.DataFrame(self.body_parts_coordinates, index=[\"x\", \"y\"]).T\n",
    "\n",
    "        self.center_of_gravity = self._compute_center_of_gravity()\n",
    "        \n",
    "        # normalize coordinates using the body length\n",
    "        self.normalized_body_parts_df = (self.body_parts_df - self.center_of_gravity) / self.length_body\n",
    "        \n",
    "\n",
    "    def _load_body_yaml_dict(self):\n",
    "        return yaml.load(Path.read_text(Path(self.body_yaml_path)), Loader=yaml.SafeLoader)\n",
    "\n",
    "    def _map_to_dict(self, arr, body_yaml_dict):\n",
    "        parts = {}\n",
    "        for k, v in body_yaml_dict.items():\n",
    "            parts[v] = arr[k, :]\n",
    "        return parts\n",
    "    \n",
    "    def _compute_center_of_gravity(self):\n",
    "        return self.body_parts_df.sum() / len(self.body_parts_df)\n",
    "\n",
    "    def _compute_length_body(self):\n",
    "        # Length of head\n",
    "        self.length_Neck_HeadTop = scipy.spatial.distance.euclidean(\n",
    "            self.body_parts_coordinates[\"Neck\"], self.body_parts_coordinates[\"HeadTop\"]\n",
    "        )\n",
    "        self.length_Neck_LEar = scipy.spatial.distance.euclidean(\n",
    "            self.body_parts_coordinates[\"Neck\"], self.body_parts_coordinates[\"LEar\"]\n",
    "        )\n",
    "        self.length_Neck_REar = scipy.spatial.distance.euclidean(\n",
    "            self.body_parts_coordinates[\"Neck\"], self.body_parts_coordinates[\"REar\"]\n",
    "        )\n",
    "        self.length_Neck_LEye = scipy.spatial.distance.euclidean(\n",
    "            self.body_parts_coordinates[\"Neck\"], self.body_parts_coordinates[\"LEye\"]\n",
    "        )\n",
    "        self.length_Neck_REye = scipy.spatial.distance.euclidean(\n",
    "            self.body_parts_coordinates[\"Neck\"], self.body_parts_coordinates[\"REye\"]\n",
    "        )\n",
    "        self.length_Nose_LEar = scipy.spatial.distance.euclidean(\n",
    "            self.body_parts_coordinates[\"Nose\"], self.body_parts_coordinates[\"LEar\"]\n",
    "        )\n",
    "        self.length_Nose_REar = scipy.spatial.distance.euclidean(\n",
    "            self.body_parts_coordinates[\"Nose\"], self.body_parts_coordinates[\"REar\"]\n",
    "        )\n",
    "        self.length_Nose_LEye = scipy.spatial.distance.euclidean(\n",
    "            self.body_parts_coordinates[\"Nose\"], self.body_parts_coordinates[\"LEye\"]\n",
    "        )\n",
    "        self.length_Nose_REye = scipy.spatial.distance.euclidean(\n",
    "            self.body_parts_coordinates[\"Nose\"], self.body_parts_coordinates[\"REye\"]\n",
    "        )\n",
    "        self.length_head = np.maximum.reduce(\n",
    "            [\n",
    "                self.length_Neck_HeadTop,\n",
    "                self.length_Neck_LEar,\n",
    "                self.length_Neck_REar,\n",
    "                self.length_Neck_LEye,\n",
    "                self.length_Neck_REye,\n",
    "                self.length_Nose_LEar,\n",
    "                self.length_Nose_REar,\n",
    "                self.length_Nose_LEye,\n",
    "                self.length_Nose_REye,\n",
    "            ]\n",
    "        )\n",
    "        # Length of torso\n",
    "        self.length_Neck_LHip = scipy.spatial.distance.euclidean(\n",
    "            self.body_parts_coordinates[\"Neck\"], self.body_parts_coordinates[\"LHip\"]\n",
    "        )\n",
    "        self.length_Neck_RHip = scipy.spatial.distance.euclidean(\n",
    "            self.body_parts_coordinates[\"Neck\"], self.body_parts_coordinates[\"RHip\"]\n",
    "        )\n",
    "        self.length_torso = np.maximum(self.length_Neck_LHip, self.length_Neck_RHip)\n",
    "\n",
    "        # Length of right leg\n",
    "        self.length_leg_right = scipy.spatial.distance.euclidean(\n",
    "            self.body_parts_coordinates[\"RHip\"], self.body_parts_coordinates[\"RKnee\"]\n",
    "        ) + scipy.spatial.distance.euclidean(\n",
    "            self.body_parts_coordinates[\"RKnee\"], self.body_parts_coordinates[\"RAnkle\"]\n",
    "        )\n",
    "\n",
    "        # Length of left leg\n",
    "        self.length_leg_left = scipy.spatial.distance.euclidean(\n",
    "            self.body_parts_coordinates[\"LHip\"], self.body_parts_coordinates[\"LKnee\"]\n",
    "        ) + scipy.spatial.distance.euclidean(\n",
    "            self.body_parts_coordinates[\"LKnee\"], self.body_parts_coordinates[\"LAnkle\"]\n",
    "        )\n",
    "\n",
    "        # Length of leg\n",
    "        self.length_leg = np.maximum(self.length_leg_right, self.length_leg_left)\n",
    "\n",
    "        # Length of body\n",
    "        self.length_body = self.length_head + self.length_torso + self.length_leg\n",
    "\n",
    "        # Check all samples have length_body of 0\n",
    "        assert (self.length_body.astype(int)) > 0, \"Length of body is 0\"\n",
    "\n",
    "    def get_normalized_body_parts_df(self):\n",
    "        return self.normalized_body_parts_df\n",
    "\n",
    "    def get_body_parts_df(self):\n",
    "        return self.body_parts_df\n",
    "\n",
    "    def get_body_parts_coordinates(self):\n",
    "        return self.body_parts_coordinates\n",
    "\n",
    "    def get_length_body(self):\n",
    "        return self.length_body\n",
    "\n",
    "    def get_length_head(self):\n",
    "        return self.length_head\n",
    "\n",
    "    def get_length_torso(self):\n",
    "        return self.length_torso\n",
    "\n",
    "    def get_length_leg_right(self):\n",
    "        return self.length_leg_right\n",
    "\n",
    "    def get_length_leg_left(self):\n",
    "        return self.length_leg_left\n",
    "\n",
    "    def get_length_leg(self):\n",
    "        return self.length_leg\n",
    "\n",
    "    def get_length_body_parts(self):\n",
    "        return self.length_body\n",
    "\n",
    "    def get_euclidean_normalized_matrix(self):\n",
    "        return pd.DataFrame(\n",
    "            scipy.spatial.distance.cdist(self.normalized_body_parts_df, self.normalized_body_parts_df),\n",
    "            columns=self.normalized_body_parts_df.index,\n",
    "            index=self.normalized_body_parts_df.index,\n",
    "        )\n",
    "\n",
    "    def get_cosine_normalized_matrix(self):\n",
    "        return pd.DataFrame(\n",
    "            scipy.spatial.distance.cdist(self.normalized_body_parts_df, self.normalized_body_parts_df, metric=\"cosine\"),\n",
    "            columns=self.normalized_body_parts_df.index,\n",
    "            index=self.normalized_body_parts_df.index,\n",
    "        )\n",
    "    \n",
    "    def get_cosine_matrix(self):\n",
    "        return pd.DataFrame(\n",
    "            scipy.spatial.distance.cdist(self.body_parts_df, self.body_parts_df, metric=\"cosine\"),\n",
    "            columns=self.body_parts_df.index,\n",
    "            index=self.body_parts_df.index,\n",
    "        )\n",
    "\n",
    "    def _calculate_angle(self, p1, p2, p3):\n",
    "        \"\"\"\n",
    "        Calculate angle between three points\n",
    "        \"\"\"\n",
    "        v1 = p2 - p1\n",
    "        v2 = p3 - p1\n",
    "        return np.rad2deg(np.arccos(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))))\n",
    "\n",
    "    def get_angle_combinations(self):\n",
    "        n_features = len(self.normalized_body_parts_df)\n",
    "        # Avoid this features:  Nose LEye REye LEar REar LElbow RElbow Neck HeadTop LBigToe LSmallToe LHeel RBigToe RSmallToe RHeel\n",
    "        avoid_features = [0, 1, 2, 3, 4, 7, 8, 17, 18, 19, 20, 21, 22, 23, 24]\n",
    "\n",
    "        length_array = [i for i in range(n_features) if i not in avoid_features]\n",
    "\n",
    "        combs = list(combinations(length_array, 3))\n",
    "\n",
    "        angles = []\n",
    "\n",
    "        for trio in combs:\n",
    "            angle = self._calculate_angle(\n",
    "                self.normalized_body_parts_df.iloc[trio[0]],\n",
    "                self.normalized_body_parts_df.iloc[trio[1]],\n",
    "                self.normalized_body_parts_df.iloc[trio[2]],\n",
    "            )\n",
    "            angles.append(angle)\n",
    "\n",
    "        named_combs = []\n",
    "        for comb in combs:\n",
    "            named_combs.append(\n",
    "                (self.body_yaml_dict[comb[0]], self.body_yaml_dict[comb[1]], self.body_yaml_dict[comb[2]])\n",
    "            )\n",
    "\n",
    "        return np.array(angles), combs, named_combs\n",
    "\n",
    "    def get_cosine_features(self):\n",
    "        \"\"\"\n",
    "        Extract cosine features from the cosine matrix. We just need one triangle and not include diagonal elements. Also the features of the face I don't think are useful.\n",
    "        \"\"\"\n",
    "        avoid_features = [0, 1, 2, 3, 4, 17, 18]  # ['Nose', 'LEye', 'REye', 'LEar', 'REar', 'Neck', 'HeadTop']\n",
    "        avoid_features = [self.body_yaml_dict[i] for i in avoid_features]\n",
    "\n",
    "        value_dict = {}\n",
    "        for i in self.get_cosine_normalized_matrix().index:\n",
    "            for j in self.get_cosine_normalized_matrix().columns:\n",
    "                if i != j:\n",
    "                    if i not in avoid_features and j not in avoid_features:\n",
    "                        if f\"{i}-{j}\" not in value_dict.keys() and f\"{j}-{i}\" not in value_dict.keys():\n",
    "                            value_dict[f\"{i}-{j}\"] = self.get_cosine_normalized_matrix().loc[i, j]\n",
    "        return value_dict\n",
    "\n",
    "    def get_euclidean_features(self):\n",
    "        \"\"\"\n",
    "        Extract cosine features from the cosine matrix. We just need one triangle and not include diagonal elements. Also the features of the face I don't think are useful.\n",
    "        \"\"\"\n",
    "        avoid_features = [0, 1, 2, 3, 4, 17, 18]  # ['Nose', 'LEye', 'REye', 'LEar', 'REar', 'Neck', 'HeadTop']\n",
    "        avoid_features = [self.body_yaml_dict[i] for i in avoid_features]\n",
    "\n",
    "        counter = 0\n",
    "        value_dict = {}\n",
    "        for i in self.get_euclidean_normalized_matrix()().index:\n",
    "            for j in self.get_euclidean_normalized_matrix()().columns:\n",
    "                if i != j:\n",
    "                    if i not in avoid_features and j not in avoid_features:\n",
    "                        if f\"{i}-{j}\" not in value_dict.keys() and f\"{j}-{i}\" not in value_dict.keys():\n",
    "                            counter += 1\n",
    "                            print(counter, f\"{i}-{j}\")\n",
    "                            value_dict[f\"{i}-{j}\"] = self.get_euclidean_normalized_matrix()().loc[i, j]\n",
    "        return value_dict\n",
    "\n",
    "    def get_position_features(self):\n",
    "        \"\"\"\n",
    "        Extract position from each body part coordinate in relation with the frame size (1920x1080)\n",
    "        \"\"\"\n",
    "        return self.get_body_parts_df() / np.array([1920, 1080])\n",
    "    \n",
    "    def save_dict(self, dict_to_save, file_name):\n",
    "        dir_path = Path('data/input_features')\n",
    "        Path.mkdir(dir_path, parents=True, exist_ok=True)\n",
    "        \n",
    "        with open(f\"{dir_path}/{file_name}.pkl\", \"wb\") as f:\n",
    "            pickle.dump(dict_to_save, f)\n",
    "        \n",
    "        print(f\"{file_name}.pkl saved\")\n",
    "        \n",
    "        \n",
    "    def save_numpy(self, numpy_to_save, file_name, dir_path='data/input_features'):\n",
    "        dir_path = Path(dir_path)\n",
    "        Path.mkdir(dir_path, parents=True, exist_ok=True)\n",
    "        \n",
    "        np.save(f\"{dir_path}/{file_name}.npy\", numpy_to_save)\n",
    "        \n",
    "        print (f\"{file_name}.npy saved\")\n",
    "        \n",
    "\n",
    "\n",
    "builder = BuildSamples(jaad_object=JAAD(\"data/jaad\"))\n",
    "data = builder.generate_sequence_samples()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eca80f5a462a1ad51b75de87be9b1c9b6e5ea7af88b13aec068ddbcf4890aa17"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('pedestrians': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
